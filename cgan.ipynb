{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "048e3206-1b59-4c33-a3f3-555ea9919fe9",
    "_uuid": "d7e8bc18-fec2-48ec-9c7b-055f28039871",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T14:48:49.827401Z",
     "iopub.status.busy": "2024-12-03T14:48:49.826134Z",
     "iopub.status.idle": "2024-12-03T14:48:49.863793Z",
     "shell.execute_reply": "2024-12-03T14:48:49.862872Z",
     "shell.execute_reply.started": "2024-12-03T14:48:49.827362Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Path to the JSON file\n",
    "json_path = '/kaggle/input/combined-landscape-captions/combined_landscape_captions.json'\n",
    "\n",
    "# Load JSON data\n",
    "with open(json_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Verify the number of samples\n",
    "print(f\"Total samples: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61a44ea3-ad38-4fd9-8594-f7722e5361e1",
    "_uuid": "9a4afe74-79e8-4f46-a563-18b059f564b6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T14:48:58.908701Z",
     "iopub.status.busy": "2024-12-03T14:48:58.907716Z",
     "iopub.status.idle": "2024-12-03T14:48:58.920537Z",
     "shell.execute_reply": "2024-12-03T14:48:58.919702Z",
     "shell.execute_reply.started": "2024-12-03T14:48:58.908609Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LandscapeDataset(Dataset):\n",
    "    def __init__(self, data, image_transform=None, tokenizer=None, max_length=128):\n",
    "        self.image_paths = list(data.keys())\n",
    "        self.captions = list(data.values())\n",
    "        self.image_transform = image_transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        caption = self.captions[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        # Tokenize caption\n",
    "        if self.tokenizer:\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                caption,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].squeeze()  # Shape: (max_length)\n",
    "            attention_mask = encoding['attention_mask'].squeeze()  # Shape: (max_length)\n",
    "        else:\n",
    "            input_ids = torch.tensor(0)\n",
    "            attention_mask = torch.tensor(0)\n",
    "\n",
    "        return {\n",
    "            'image': image,                  # Tensor: (3, H, W)\n",
    "            'input_ids': input_ids,          # Tensor: (max_length)\n",
    "            'attention_mask': attention_mask,  # Tensor: (max_length)\n",
    "            'caption': caption\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "707b0ce8-724f-413d-a168-0cbbfe98bf11",
    "_uuid": "e9093f46-649a-4d2f-b800-b4a4b1d7d0d0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T14:49:08.586491Z",
     "iopub.status.busy": "2024-12-03T14:49:08.586137Z",
     "iopub.status.idle": "2024-12-03T14:49:08.763503Z",
     "shell.execute_reply": "2024-12-03T14:49:08.762698Z",
     "shell.execute_reply.started": "2024-12-03T14:49:08.586460Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # You can adjust the size based on your model's requirements\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],  # Normalizing between -1 and 1\n",
    "                         std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c8e5c2b8-6e24-4d33-af23-ef309c69fc70",
    "_uuid": "989c650e-fcab-4308-b3a1-47603b7444aa",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T14:49:21.287864Z",
     "iopub.status.busy": "2024-12-03T14:49:21.287133Z",
     "iopub.status.idle": "2024-12-03T14:52:04.172734Z",
     "shell.execute_reply": "2024-12-03T14:52:04.171707Z",
     "shell.execute_reply.started": "2024-12-03T14:49:21.287825Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = LandscapeDataset(data, image_transform=image_transform, tokenizer=tokenizer)\n",
    "\n",
    "# Remove None samples if any\n",
    "dataset = [sample for sample in dataset if sample is not None]\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32  # Adjust based on your GPU memory\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6029c9b2-de1d-4d63-bf5e-4513421ba247",
    "_uuid": "ba7147ef-29ce-416d-ab84-ef813761a0f3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T14:52:23.163434Z",
     "iopub.status.busy": "2024-12-03T14:52:23.163069Z",
     "iopub.status.idle": "2024-12-03T14:52:23.169491Z",
     "shell.execute_reply": "2024-12-03T14:52:23.168537Z",
     "shell.execute_reply.started": "2024-12-03T14:52:23.163403Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Text embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Create text encoder (we can use a simple embedding layer for this example)\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, embed_dim, batch_first=True)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        x = self.embedding(input_ids)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return hidden.squeeze(0)  # Shape: (batch_size, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "642b3cbd-b044-4dfb-b603-32364d84001f",
    "_uuid": "11d021df-6d52-4983-a815-a37a6ce3407f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T14:52:31.483487Z",
     "iopub.status.busy": "2024-12-03T14:52:31.483122Z",
     "iopub.status.idle": "2024-12-03T14:52:31.491662Z",
     "shell.execute_reply": "2024-12-03T14:52:31.490680Z",
     "shell.execute_reply.started": "2024-12-03T14:52:31.483456Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, embed_dim, feature_maps=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.noise_dim = noise_dim\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(noise_dim + embed_dim, feature_maps * 8 * 4 * 4),\n",
    "            nn.BatchNorm1d(feature_maps * 8 * 4 * 4),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            # Input: (feature_maps * 8) x 4 x 4\n",
    "            nn.ConvTranspose2d(feature_maps * 8, feature_maps * 4, 4, 2, 1),  # 8x8\n",
    "            nn.BatchNorm2d(feature_maps * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(feature_maps * 4, feature_maps * 2, 4, 2, 1),  # 16x16\n",
    "            nn.BatchNorm2d(feature_maps * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(feature_maps * 2, feature_maps, 4, 2, 1),  # 32x32\n",
    "            nn.BatchNorm2d(feature_maps),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(feature_maps, 3, 4, 2, 1),  # 64x64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text_embedding):\n",
    "        x = torch.cat((noise, text_embedding), dim=1)  # Shape: (batch_size, noise_dim + embed_dim)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 512, 4, 4)\n",
    "        img = self.deconv(x)\n",
    "        return img  # Shape: (batch_size, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d3bd2332-ae40-48d7-a408-8cf3a79beb35",
    "_uuid": "89339873-f2a4-4b26-982a-60bb670e2fa0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T15:00:10.095271Z",
     "iopub.status.busy": "2024-12-03T15:00:10.094527Z",
     "iopub.status.idle": "2024-12-03T15:00:10.104893Z",
     "shell.execute_reply": "2024-12-03T15:00:10.104111Z",
     "shell.execute_reply.started": "2024-12-03T15:00:10.095233Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_dim, feature_maps=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.feature_maps = feature_maps  # Store feature_maps as an instance variable\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # Input: (3) x 64 x 64\n",
    "            nn.Conv2d(3, self.feature_maps, 4, 2, 1),  # 32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(self.feature_maps, self.feature_maps * 2, 4, 2, 1),  # 16x16\n",
    "            nn.BatchNorm2d(self.feature_maps * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(self.feature_maps * 2, self.feature_maps * 4, 4, 2, 1),  # 8x8\n",
    "            nn.BatchNorm2d(self.feature_maps * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(self.feature_maps * 4, self.feature_maps * 8, 4, 2, 1),  # 4x4\n",
    "            nn.BatchNorm2d(self.feature_maps * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.text_projection = nn.Linear(embed_dim, self.feature_maps * 8 * 4 * 4)\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Conv2d(self.feature_maps * 8, 1, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, text_embedding):\n",
    "        x_img = self.conv(img)  # Shape: (batch_size, feature_maps * 8, 4, 4)\n",
    "        x_text = self.text_projection(text_embedding)  # Shape: (batch_size, feature_maps * 8 * 4 * 4)\n",
    "        x_text = x_text.view(-1, self.feature_maps * 8, 4, 4)  # Use self.feature_maps\n",
    "\n",
    "        # Element-wise addition\n",
    "        x = x_img + x_text\n",
    "\n",
    "        out = self.output(x).view(-1)\n",
    "        return out  # Shape: (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "50782924-0862-4302-aba0-9ef7ef741cb3",
    "_uuid": "e3b6782f-69a5-4ca6-ad34-27f9c7b44878",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T14:59:18.855676Z",
     "iopub.status.busy": "2024-12-03T14:59:18.855161Z",
     "iopub.status.idle": "2024-12-03T14:59:19.084421Z",
     "shell.execute_reply": "2024-12-03T14:59:19.083416Z",
     "shell.execute_reply.started": "2024-12-03T14:59:18.855620Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Initialize models\n",
    "vocab_size = tokenizer.vocab_size\n",
    "text_encoder = TextEncoder(vocab_size, embedding_dim)\n",
    "generator = Generator(noise_dim=100, embed_dim=embedding_dim)\n",
    "discriminator = Discriminator(embed_dim=embedding_dim, feature_maps=64)\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "text_encoder = text_encoder.to(device)\n",
    "generator = generator.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_TE = optim.Adam(text_encoder.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2aad340f-ec20-44fe-a64f-a37f99c03319",
    "_uuid": "646eda07-b203-4f8c-8542-22277fe2006b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T15:12:34.137670Z",
     "iopub.status.busy": "2024-12-03T15:12:34.136509Z",
     "iopub.status.idle": "2024-12-03T15:35:16.776035Z",
     "shell.execute_reply": "2024-12-03T15:35:16.774946Z",
     "shell.execute_reply.started": "2024-12-03T15:12:34.137604Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_gan(train_loader, text_encoder, generator, discriminator, \n",
    "              optimizer_G, optimizer_D, optimizer_TE, criterion, device, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        text_encoder.train()\n",
    "\n",
    "        total_d_loss = 0\n",
    "        total_g_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "\n",
    "            # Generate labels\n",
    "            real_labels = torch.ones(batch_size).to(device)\n",
    "            fake_labels = torch.zeros(batch_size).to(device)\n",
    "\n",
    "            # Train Text Encoder and get text embeddings\n",
    "            optimizer_TE.zero_grad()\n",
    "            text_embeddings = text_encoder(input_ids)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real images\n",
    "            real_outputs = discriminator(images, text_embeddings)\n",
    "            d_loss_real = criterion(real_outputs, real_labels)\n",
    "            \n",
    "            # Fake images\n",
    "            noise = torch.randn(batch_size, 100).to(device)\n",
    "            with torch.no_grad():\n",
    "                fake_images = generator(noise, text_embeddings.detach())\n",
    "            \n",
    "            fake_outputs = discriminator(fake_images.detach(), text_embeddings.detach())\n",
    "            d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Regenerate fake images with gradient flow\n",
    "            noise = torch.randn(batch_size, 100).to(device)\n",
    "            fake_images = generator(noise, text_embeddings)\n",
    "            g_outputs = discriminator(fake_images, text_embeddings)\n",
    "            g_loss = criterion(g_outputs, real_labels)\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Optionally update text encoder\n",
    "            optimizer_TE.step()\n",
    "\n",
    "            # Accumulate losses\n",
    "            total_d_loss += d_loss.item()\n",
    "            total_g_loss += g_loss.item()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Avg Loss D: {total_d_loss/len(train_loader):.4f}, Avg Loss G: {total_g_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Save models after each epoch\n",
    "        torch.save(generator.state_dict(), f'generator_epoch_{epoch+1}.pth')\n",
    "        torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch+1}.pth')\n",
    "        torch.save(text_encoder.state_dict(), f'text_encoder_epoch_{epoch+1}.pth')\n",
    "\n",
    "# Usage in main training script\n",
    "train_gan(train_loader, text_encoder, generator, discriminator, \n",
    "          optimizer_G, optimizer_D, optimizer_TE, criterion, device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "990869ab-3364-4768-8f6d-692e3a801a97",
    "_uuid": "9a7e5e66-7a8f-487d-aadd-d549aa99b465",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-12-03T16:06:58.670229Z",
     "iopub.status.busy": "2024-12-03T16:06:58.669814Z",
     "iopub.status.idle": "2024-12-03T16:06:58.702517Z",
     "shell.execute_reply": "2024-12-03T16:06:58.701433Z",
     "shell.execute_reply.started": "2024-12-03T16:06:58.670196Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_images(generator, text_encoder, tokenizer, text_list, device, noise_dim=100):\n",
    "    generator.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text in text_list:\n",
    "            # Tokenize text\n",
    "            encoding = tokenizer.encode_plus(\n",
    "                text,\n",
    "                add_special_tokens=True,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            # Get text embedding\n",
    "            text_embedding = text_encoder(input_ids)  # Shape: (1, embed_dim)\n",
    "\n",
    "            # Generate noise\n",
    "            noise = torch.randn(1, noise_dim).to(device)\n",
    "\n",
    "            # Generate image\n",
    "            fake_image = generator(noise, text_embedding)\n",
    "            fake_image = fake_image.squeeze(0)\n",
    "            fake_image = (fake_image * 0.5) + 0.5  # Denormalize to [0,1]\n",
    "            fake_image = fake_image.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "            # Display image\n",
    "            plt.imshow(fake_image)\n",
    "            plt.title(text)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "# Example usage\n",
    "sample_texts = [\n",
    "    \"a beach with sea\",\n",
    "    \"a mountain range in the distance\",\n",
    "    \"a forest with moss and trees\",\n",
    "    \"a desert landscape with sand dunes\",\n",
    "    \"a glacier\"\n",
    "]\n",
    "\n",
    "generate_images(generator, text_encoder, tokenizer, sample_texts, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c9f254b-4273-403b-9587-a75bb37e398c",
    "_uuid": "a6ecbc90-3464-4105-a2f2-29e3bf77ecef",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 298806,
     "sourceId": 1217826,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2707450,
     "sourceId": 4669588,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6221013,
     "sourceId": 10089158,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
